{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading # 因為等等要載入大量資料所以要multi thread\n",
    "from multiprocessing import Queue\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.family']='SimHei' #顯示中文\n",
    "\n",
    "#特殊符號: | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx = pd.DataFrame({'data1':np.arange(5)})\\ny = pd.Series([1,0,0,1,0])\\n\\nfrom sklearn.utils import shuffle \\n\\ndef train_batch_generator(x, y, bs):\\n    badIndex = y[y == 1].index\\n    goodIndex = y[y == 0].index\\n    print('bad index length : ' , len(badIndex))\\n    print('good index length : ' , len(goodIndex))\\n\\n    badCnt = 0\\n    goodCnt = 0\\n    while(True):\\n        batch_x = pd.DataFrame()\\n        batch_y = pd.Series()\\n\\n        for i in range(int(bs/2)):\\n            if(badCnt >= len(badIndex)):\\n                badIndex = shuffle(badIndex)\\n                badCnt = 0\\n\\n            batch_x = batch_x.append(x.loc[badIndex[badCnt]], ignore_index=True)\\n            batch_y = batch_y.append(pd.Series(y.loc[badIndex[badCnt]]), ignore_index=True)\\n            badCnt += 1\\n\\n        for i in range(int(bs/2)):\\n            if(goodCnt >= len(goodIndex)):\\n                goodIndex = shuffle(goodIndex)\\n                goodCnt = 0\\n\\n            batch_x = batch_x.append(x.loc[goodIndex[goodCnt]], ignore_index=True)\\n            batch_y = batch_y.append(pd.Series(y.loc[goodIndex[goodCnt]]), ignore_index=True)\\n            goodCnt += 1\\n\\n        yield batch_x, batch_y\\n        \\ngen = train_batch_generator(x, y, 3)\\nnext(gen)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = pd.DataFrame({'data1':np.arange(5)})\n",
    "y = pd.Series([1,0,0,1,0])\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "def train_batch_generator(x, y, bs):\n",
    "    badIndex = y[y == 1].index\n",
    "    goodIndex = y[y == 0].index\n",
    "    print('bad index length : ' , len(badIndex))\n",
    "    print('good index length : ' , len(goodIndex))\n",
    "\n",
    "    badCnt = 0\n",
    "    goodCnt = 0\n",
    "    while(True):\n",
    "        batch_x = pd.DataFrame()\n",
    "        batch_y = pd.Series()\n",
    "\n",
    "        for i in range(int(bs/2)):\n",
    "            if(badCnt >= len(badIndex)):\n",
    "                badIndex = shuffle(badIndex)\n",
    "                badCnt = 0\n",
    "\n",
    "            batch_x = batch_x.append(x.loc[badIndex[badCnt]], ignore_index=True)\n",
    "            batch_y = batch_y.append(pd.Series(y.loc[badIndex[badCnt]]), ignore_index=True)\n",
    "            badCnt += 1\n",
    "\n",
    "        for i in range(int(bs/2)):\n",
    "            if(goodCnt >= len(goodIndex)):\n",
    "                goodIndex = shuffle(goodIndex)\n",
    "                goodCnt = 0\n",
    "\n",
    "            batch_x = batch_x.append(x.loc[goodIndex[goodCnt]], ignore_index=True)\n",
    "            batch_y = batch_y.append(pd.Series(y.loc[goodIndex[goodCnt]]), ignore_index=True)\n",
    "            goodCnt += 1\n",
    "\n",
    "        yield batch_x, batch_y\n",
    "        \n",
    "gen = train_batch_generator(x, y, 3)\n",
    "next(gen)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [FileID, label]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [FileID, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 載入資料 A\n",
    "\n",
    "train = pd.read_csv('data/training-set.csv', encoding = \"utf-8\", header=None)\n",
    "test = pd.read_csv('data/testing-set.csv', encoding = \"utf-8\", header=None)\n",
    "\n",
    "train_exc = pd.read_csv('data/exception/exception_train.txt', encoding = \"utf-8\", header=None)\n",
    "test_exc = pd.read_csv('data/exception/exception_testing.txt', encoding = \"utf-8\", header=None)\n",
    "\n",
    "train.columns=['FileID','label']\n",
    "test.columns=['FileID','label']\n",
    "\n",
    "\n",
    "# 確認排除的FileID在training set裡面找不到\n",
    "\n",
    "for item in train_exc:\n",
    "    print(train[train['FileID']==item])\n",
    "    \n",
    "for item in test_exc:\n",
    "    print(test[test['FileID']==item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from multiprocessing import Queue    #使用多核心的模組 Queue\n",
    "#import time\n",
    "\n",
    "#list1=[]\n",
    "\n",
    "#def func(i):\n",
    " #   time.sleep(np.random.randint(0,5))\n",
    "  #  list1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = '/data/examples/trend/data/query_log'\n",
    "\n",
    "def ReadFile(fp):\n",
    "    return pd.read_csv(\n",
    "        fp, \n",
    "        names=['FileID','CustomerID','QueryTs','ProductID'], \n",
    "        dtype={'FileID': np.str, 'CustomerID': np.str, 'QueryTs': np.int32, 'ProductID': np.str} )\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) # advisable to use os.path.join as this makes concatenation OS independent\n",
    "file = [ReadFile(f) for f in all_files]\n",
    "raw  = pd.concat(file, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfiles = [f[0:4] for f in os.listdir(\\'/data/examples/trend/data/query_log\\')] # 存取每日檔案的檔案夾\\n\\nraw = pd.DataFrame()\\ndef job(num, q, lock): \\n\\n    lock.acquire()    #鎖上這個線程，在完成之前不讓其他線程干擾變數。\\n    global raw\\n    for i in range(num): # 使用 hub 時才用上真正data : len(files)\\n \\n        unit = pd.read_csv(\\'/data/examples/trend/data/query_log/\\' + str(files[i]) + \\'.csv\\', encoding = \"utf-8\", header=None)\\n        unit.columns=[\\'FileID\\',\\'CustomerID\\',\\'QueryTs\\',\\'ProductID\\']\\n        raw = raw.append(unit, ignore_index=True)\\n    \\n    lock.release() #解鎖這個線程，開始其他線程。\\n    \\nlock = threading.Lock()  #命名一個 Lock 物件\\nq = Queue() # 開一個 Queue 物件\\n\\nt1 = threading.Thread(target=job, args=(len(files),q,lock)) \\n                                                        #打開一個名字叫 t1 的線程物件，呼叫 job\\n                                                        #同時t1導入 q 跟 lock 控制線程\\n                                                        #Thread 裡面的 function 不可以有 return, 不然會出錯。\\n\\nt1.start() #啟動 t1 線程 # t2.start() \\nt1.join()  #在 t1 線程結束前阻止程式繼續運行\\n\\nprint(raw.shape)\\nraw.head()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "files = [f[0:4] for f in os.listdir('/data/examples/trend/data/query_log')] # 存取每日檔案的檔案夾\n",
    "\n",
    "raw = pd.DataFrame()\n",
    "def job(num, q, lock): \n",
    "\n",
    "    lock.acquire()    #鎖上這個線程，在完成之前不讓其他線程干擾變數。\n",
    "    global raw\n",
    "    for i in range(num): # 使用 hub 時才用上真正data : len(files)\n",
    " \n",
    "        unit = pd.read_csv('/data/examples/trend/data/query_log/' + str(files[i]) + '.csv', encoding = \"utf-8\", header=None)\n",
    "        unit.columns=['FileID','CustomerID','QueryTs','ProductID']\n",
    "        raw = raw.append(unit, ignore_index=True)\n",
    "    \n",
    "    lock.release() #解鎖這個線程，開始其他線程。\n",
    "    \n",
    "lock = threading.Lock()  #命名一個 Lock 物件\n",
    "q = Queue() # 開一個 Queue 物件\n",
    "\n",
    "t1 = threading.Thread(target=job, args=(len(files),q,lock)) \n",
    "                                                        #打開一個名字叫 t1 的線程物件，呼叫 job\n",
    "                                                        #同時t1導入 q 跟 lock 控制線程\n",
    "                                                        #Thread 裡面的 function 不可以有 return, 不然會出錯。\n",
    "\n",
    "t1.start() #啟動 t1 線程 # t2.start() \n",
    "t1.join()  #在 t1 線程結束前阻止程式繼續運行\n",
    "\n",
    "print(raw.shape)\n",
    "raw.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83273110, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTs</th>\n",
       "      <th>ProductID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63676131ae4f15699db65bd7f9a2a18a</td>\n",
       "      <td>49075aad975fa57e805c62a8494a75e4</td>\n",
       "      <td>1495497600</td>\n",
       "      <td>c105a0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ba04807b9b65835482302fab98fc523f</td>\n",
       "      <td>da02366776ceaae99adeed9c7140307f</td>\n",
       "      <td>1495497600</td>\n",
       "      <td>c105a0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16ea975c524b69ca3f2561969fabed6b</td>\n",
       "      <td>437276bc51f1603b0c8b41119019d813</td>\n",
       "      <td>1495497604</td>\n",
       "      <td>c76d58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1941c3fe0c5c2d53ba94d0eab99f169c</td>\n",
       "      <td>a286ad2bfd4a05d98a3a393b21180aaf</td>\n",
       "      <td>1495497612</td>\n",
       "      <td>c105a0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1941c3fe0c5c2d53ba94d0eab99f169c</td>\n",
       "      <td>a286ad2bfd4a05d98a3a393b21180aaf</td>\n",
       "      <td>1495497615</td>\n",
       "      <td>c105a0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID                        CustomerID  \\\n",
       "0  63676131ae4f15699db65bd7f9a2a18a  49075aad975fa57e805c62a8494a75e4   \n",
       "1  ba04807b9b65835482302fab98fc523f  da02366776ceaae99adeed9c7140307f   \n",
       "2  16ea975c524b69ca3f2561969fabed6b  437276bc51f1603b0c8b41119019d813   \n",
       "3  1941c3fe0c5c2d53ba94d0eab99f169c  a286ad2bfd4a05d98a3a393b21180aaf   \n",
       "4  1941c3fe0c5c2d53ba94d0eab99f169c  a286ad2bfd4a05d98a3a393b21180aaf   \n",
       "\n",
       "      QueryTs ProductID  \n",
       "0  1495497600    c105a0  \n",
       "1  1495497600    c105a0  \n",
       "2  1495497604    c76d58  \n",
       "3  1495497612    c105a0  \n",
       "4  1495497615    c105a0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw.shape)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83273110, 9) (52518, 2) (54250245, 10)\n",
      "['7acab3' '055649' '634e6b' 'c76d58' 'c105a0' 'e47f04' '885fab' '26a5d0'\n",
      " 'a310bb' 'dd8d4a' 'd465fc' '533133' '262880' 'b93794' '8541a0' '218578'\n",
      " '3ea8c3' '05b409' '20f8a5' '0374c4' 'cc3a6a' '8452da' 'aaa9c8' '0cdb7a'\n",
      " '3c2be6' '75f310' 'fec24f']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTs</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>cnt</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>week</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6872722</th>\n",
       "      <td>f096e1c6e0cbaf10389fbf427b4d341f</td>\n",
       "      <td>0000006fa286976bf35ea17f1f19bc7a</td>\n",
       "      <td>1493364274</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>07:24:34</td>\n",
       "      <td>7</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873003</th>\n",
       "      <td>f096e1c6e0cbaf10389fbf427b4d341f</td>\n",
       "      <td>0000006fa286976bf35ea17f1f19bc7a</td>\n",
       "      <td>1493531993</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>05:59:53</td>\n",
       "      <td>5</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119508</th>\n",
       "      <td>19308434813502167aaef38f578981a4</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1490544224</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>16:03:44</td>\n",
       "      <td>16</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156382</th>\n",
       "      <td>ee6a1280be5c96d7b2461de6b7578180</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1492708112</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>17:08:32</td>\n",
       "      <td>17</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32158918</th>\n",
       "      <td>ee6a1280be5c96d7b2461de6b7578180</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1492962863</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>15:54:23</td>\n",
       "      <td>15</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    FileID                        CustomerID  \\\n",
       "6872722   f096e1c6e0cbaf10389fbf427b4d341f  0000006fa286976bf35ea17f1f19bc7a   \n",
       "6873003   f096e1c6e0cbaf10389fbf427b4d341f  0000006fa286976bf35ea17f1f19bc7a   \n",
       "119508    19308434813502167aaef38f578981a4  00000145d9062eada528bace5fb4864e   \n",
       "32156382  ee6a1280be5c96d7b2461de6b7578180  00000145d9062eada528bace5fb4864e   \n",
       "32158918  ee6a1280be5c96d7b2461de6b7578180  00000145d9062eada528bace5fb4864e   \n",
       "\n",
       "             QueryTs ProductID  cnt        date      time  hour      week  \\\n",
       "6872722   1493364274    7acab3    1  2017-04-28  07:24:34     7    Friday   \n",
       "6873003   1493531993    7acab3    1  2017-04-30  05:59:53     5    Sunday   \n",
       "119508    1490544224    7acab3    1  2017-03-26  16:03:44    16    Sunday   \n",
       "32156382  1492708112    7acab3    1  2017-04-20  17:08:32    17  Thursday   \n",
       "32158918  1492962863    7acab3    1  2017-04-23  15:54:23    15    Sunday   \n",
       "\n",
       "          label  \n",
       "6872722       0  \n",
       "6873003       0  \n",
       "119508        0  \n",
       "32156382      0  \n",
       "32158918      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 載入資料 B \n",
    "'''\n",
    "files = [f[0:4] for f in os.listdir('/data/examples/trend/data/query_log')] # 存取每日檔案的檔案夾\n",
    "\n",
    "raw = pd.DataFrame()\n",
    "\n",
    "for i in range(3): # 使用 hub 時才用上真正data : len(files)\n",
    " \n",
    "    unit = pd.read_csv('/data/examples/trend/data/query_log/' + str(files[i]) + '.csv', encoding = \"utf-8\", header=None)\n",
    "    unit.columns=['FileID','CustomerID','QueryTs','ProductID']\n",
    "    raw = raw.append(unit, ignore_index=True)\n",
    "'''\n",
    "\n",
    "raw['cnt'] = 1\n",
    "raw['date'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.date\n",
    "raw['time'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.time\n",
    "raw['hour'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.hour\n",
    "raw['week'] = pd.to_datetime(raw['QueryTs'], unit='s').dt.weekday_name\n",
    "\n",
    "#raw.tail()\n",
    "\n",
    "train_raw = pd.merge(raw, train, on=['FileID'])\n",
    "train_raw['ProductID'] = train_raw['ProductID'].astype(str) # 因為值中有數字文字\n",
    "train_raw.replace({'ProductID':{'55649': '055649'}}) # 因為raw data有少值\n",
    "train_raw = train_raw.sort_values(['CustomerID','FileID','QueryTs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83273110, 9) (52518, 2) (54250245, 10)\n",
      "['7acab3' '055649' '634e6b' 'c76d58' 'c105a0' 'e47f04' '885fab' '26a5d0'\n",
      " 'a310bb' 'dd8d4a' 'd465fc' '533133' '262880' 'b93794' '8541a0' '218578'\n",
      " '3ea8c3' '05b409' '20f8a5' '0374c4' 'cc3a6a' '8452da' 'aaa9c8' '0cdb7a'\n",
      " '3c2be6' '75f310' 'fec24f']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTs</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>cnt</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>week</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6872722</th>\n",
       "      <td>f096e1c6e0cbaf10389fbf427b4d341f</td>\n",
       "      <td>0000006fa286976bf35ea17f1f19bc7a</td>\n",
       "      <td>1493364274</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>07:24:34</td>\n",
       "      <td>7</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873003</th>\n",
       "      <td>f096e1c6e0cbaf10389fbf427b4d341f</td>\n",
       "      <td>0000006fa286976bf35ea17f1f19bc7a</td>\n",
       "      <td>1493531993</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>05:59:53</td>\n",
       "      <td>5</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119508</th>\n",
       "      <td>19308434813502167aaef38f578981a4</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1490544224</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>16:03:44</td>\n",
       "      <td>16</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156382</th>\n",
       "      <td>ee6a1280be5c96d7b2461de6b7578180</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1492708112</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>17:08:32</td>\n",
       "      <td>17</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32158918</th>\n",
       "      <td>ee6a1280be5c96d7b2461de6b7578180</td>\n",
       "      <td>00000145d9062eada528bace5fb4864e</td>\n",
       "      <td>1492962863</td>\n",
       "      <td>7acab3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>15:54:23</td>\n",
       "      <td>15</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    FileID                        CustomerID  \\\n",
       "6872722   f096e1c6e0cbaf10389fbf427b4d341f  0000006fa286976bf35ea17f1f19bc7a   \n",
       "6873003   f096e1c6e0cbaf10389fbf427b4d341f  0000006fa286976bf35ea17f1f19bc7a   \n",
       "119508    19308434813502167aaef38f578981a4  00000145d9062eada528bace5fb4864e   \n",
       "32156382  ee6a1280be5c96d7b2461de6b7578180  00000145d9062eada528bace5fb4864e   \n",
       "32158918  ee6a1280be5c96d7b2461de6b7578180  00000145d9062eada528bace5fb4864e   \n",
       "\n",
       "             QueryTs ProductID  cnt        date      time  hour      week  \\\n",
       "6872722   1493364274    7acab3    1  2017-04-28  07:24:34     7    Friday   \n",
       "6873003   1493531993    7acab3    1  2017-04-30  05:59:53     5    Sunday   \n",
       "119508    1490544224    7acab3    1  2017-03-26  16:03:44    16    Sunday   \n",
       "32156382  1492708112    7acab3    1  2017-04-20  17:08:32    17  Thursday   \n",
       "32158918  1492962863    7acab3    1  2017-04-23  15:54:23    15    Sunday   \n",
       "\n",
       "          label  \n",
       "6872722       0  \n",
       "6873003       0  \n",
       "119508        0  \n",
       "32156382      0  \n",
       "32158918      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw.shape, train.shape, train_raw.shape)\n",
    "print(train_raw.ProductID.unique())\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.to_csv('train_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nobserve1 = train_raw.groupby(['FileID','CustomerID','ProductID'])[['cnt','label']].sum()\\nobserve1 = observe1.sort_values('label', ascending=0)\\naa = observe1.loc[observe1['label']==1]\\naa.head()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "observe1 = train_raw.groupby(['FileID','CustomerID','ProductID'])[['cnt','label']].sum()\n",
    "observe1 = observe1.sort_values('label', ascending=0)\n",
    "aa = observe1.loc[observe1['label']==1]\n",
    "aa.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nobserve2 = train_raw.groupby(['week'])[['cnt','label']].sum() \\nobserve2['fr_rate'] = observe2['label'] / observe2['cnt']\\nobserve2 = observe2.sort_values('fr_rate', ascending=0)\\nprint(observe2)\\n\\nobserve3 = train_raw.groupby(['ProductID'])[['cnt','label']].sum() \\nobserve3['fr_rate'] = observe3['label'] / observe3['cnt']\\nobserve3 = observe3.sort_values('fr_rate', ascending=0)\\nprint(observe3)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "observe2 = train_raw.groupby(['week'])[['cnt','label']].sum() \n",
    "observe2['fr_rate'] = observe2['label'] / observe2['cnt']\n",
    "observe2 = observe2.sort_values('fr_rate', ascending=0)\n",
    "print(observe2)\n",
    "\n",
    "observe3 = train_raw.groupby(['ProductID'])[['cnt','label']].sum() \n",
    "observe3['fr_rate'] = observe3['label'] / observe3['cnt']\n",
    "observe3 = observe3.sort_values('fr_rate', ascending=0)\n",
    "print(observe3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2A. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aggregate\n",
    "\n",
    "# 單日File被使用幾次\n",
    "DayFil    = train_raw.groupby(['FileID', 'date']).size()\n",
    "DayFilMax = DayFil.groupby(level=0).max()\n",
    "DayFilMin = DayFil.groupby(level=0).min()\n",
    "DayFilMea = DayFil.groupby(level=0).mean()\n",
    "\n",
    "\n",
    "DayCs  = train_raw.groupby(['FileID', 'date', 'CustomerID']).size()\n",
    "DayPr  = train_raw.groupby(['FileID', 'date', 'ProductID']).size()\n",
    "Pr     = train_raw.groupby(['FileID', 'ProductID']).size()\n",
    "Cs     = train_raw.groupby(['FileID', 'CustomerID']).size()\n",
    "Day    = train_raw.groupby(['FileID', 'date']).size()\n",
    "\n",
    "# 單日File被多少客人使用\n",
    "DayTCs    = DayCs.groupby(level=(0,1)).size()\n",
    "DayTCsMax = DayTCs.groupby(level=0).max()\n",
    "DayTCsMin = DayTCs.groupby(level=0).min()\n",
    "DayTCsMea = DayTCs.groupby(level=0).mean()\n",
    "\n",
    "# 單日File被同一客人使用幾次\n",
    "DaySCsMax = DayCs.groupby(level=0).max()\n",
    "DaySCsMin = DayCs.groupby(level=0).min()\n",
    "DaySCsMea = DayCs.groupby(level=0).mean()\n",
    "\n",
    "# 單日File被多少產品使用\n",
    "DayTPr    = DayPr.groupby(level=(0,1)).size()\n",
    "DayTPrMax = DayTPr.groupby(level=0).max()\n",
    "DayTPrMin = DayTPr.groupby(level=0).min()\n",
    "DayTPrMea = DayTPr.groupby(level=0).mean()\n",
    "\n",
    "# 單日File被同一產品使用幾次\n",
    "DaySPrMax = DayPr.groupby(level=0).max()\n",
    "DaySPrMin = DayPr.groupby(level=0).min()\n",
    "DaySPrMea = DayPr.groupby(level=0).mean()\n",
    "\n",
    "\n",
    "TCs    = Cs.groupby(level=(0)).size() # 總共File被多少客人使用\n",
    "TPr    = Pr.groupby(level=(0)).size() # 總共File被多少產品使用\n",
    "Day    = Day.groupby(level=(0)).size() # 總共File被使用幾天\n",
    "Fil    = train_raw.groupby(['FileID']).size() # 總共File被使用幾次\n",
    "\n",
    "train_agg = pd.concat([DayFilMax, DayFilMin, DayFilMea,\n",
    "                       DayTCsMax, DayTCsMin, DayTCsMea,\n",
    "                       DaySCsMax, DaySCsMin, DaySCsMea,\n",
    "                       DayTPrMax, DayTPrMin, DayTPrMea,\n",
    "                       DaySPrMax, DaySPrMin, DaySPrMea,\n",
    "                       TCs,    TPr,   Day,    Fil,     ], axis=1)\n",
    "\n",
    "train_agg.columns = ['DayFilMax', 'DayFilMin', 'DayFilMea',\n",
    "                     'DayTCsMax', 'DayTCsMin', 'DayTCsMea',\n",
    "                     'DaySCsMax', 'DaySCsMin', 'DaySCsMea',\n",
    "                     'DayTPrMax', 'DayTPrMin', 'DayTPrMea',\n",
    "                     'DaySPrMax', 'DaySPrMin', 'DaySPrMea',\n",
    "                     'TCs',   'TPr',  'Day',  'Fil'      ]\n",
    "\n",
    "train_agg = pd.DataFrame(train_agg).reset_index()\n",
    "train_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "Pr = train_raw[['FileID', 'ProductID']]\n",
    "Hr = train_raw[['FileID', 'hour']]\n",
    "Wk = train_raw[['FileID', 'week']]\n",
    "\n",
    "train_prd = pd.concat([Pr, pd.get_dummies(Pr.ProductID)], 1).groupby(['FileID']).sum().reset_index()\n",
    "train_hur = pd.concat([Hr, pd.get_dummies(Hr.hour)], 1).groupby(['FileID']).sum().reset_index()\n",
    "train_wek = pd.concat([Wk, pd.get_dummies(Wk.week)], 1).groupby(['FileID']).sum().reset_index()\n",
    "\n",
    "train_dum = pd.merge(train_prd, train_hur, on='FileID')\n",
    "train_dum = pd.merge(train_dum, train_wek, on='FileID')\n",
    "\n",
    "train_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用間隔\n",
    "\n",
    "def regular(x):\n",
    "    return ( x%3600 <= 10 ) | ( x%3600 >= 3590 )\n",
    "\n",
    "#print(regular(np.array([1,3599,3601,450,720,4150])))\n",
    "\n",
    "def outlier(x):\n",
    "    Q1 = np.percentile(x, 25)\n",
    "    Q3 = np.percentile(x, 75)\n",
    "    h = Q3 - Q1\n",
    "    return ((Q3 + 0.5*h) >= x) & (x >= (Q1 - 0.5*h))\n",
    "\n",
    "#print(outlier(np.array([1,100,100,100,100,100,100])))\n",
    "\n",
    "train_freq = train_raw[['CustomerID','FileID','QueryTs','label']] # .head(9000)\n",
    "\n",
    "train_freq['Delta1']    = train_freq.groupby(['FileID'])['QueryTs'].transform(lambda x: x-x.shift(1))\n",
    "train_freq['Delta1_d']  = train_freq.groupby(['FileID'])['Delta1'].transform(lambda x: abs(x-x.shift(1)))\n",
    "#train_freq['Delta2']    = train_freq.groupby(['CustomerID','FileID'])['QueryTs'].transform(lambda x: x-x.shift(1))\n",
    "#train_freq['Delta2_d']  = train_freq.groupby(['CustomerID','FileID'])['Delta2'].transform(lambda x: abs(x-x.shift(1)))\n",
    "\n",
    "# 測: 是否為3600秒一單位)\n",
    "\n",
    "train_freq1 = train_freq.drop(['Delta1_d'], axis=1).dropna() #.loc[train_freq['label']==0]\n",
    "train_freq1['Regular'] = train_freq1['Delta1'].apply(regular)\n",
    "train_freq1mean = train_freq1.groupby(['FileID'])['Regular'].mean().reset_index()\n",
    "train_freq1max  = train_freq1.groupby(['FileID'])['Regular'].max().reset_index()\n",
    "\n",
    "# 測: 規律性 (標準差)\n",
    "\n",
    "train_freq2 = train_freq.dropna() # .loc[train_freq['label']==0]\n",
    "train_freq2['Delta_OL'] = train_freq2.groupby(['CustomerID','FileID'])['Delta1_d'].transform(outlier)\n",
    "train_freq2 = train_freq2[train_freq2['Delta_OL'] != 0]\n",
    "train_freq2['Delta_sd'] = train_freq2.groupby(['CustomerID','FileID'])['Delta1_d'].transform(lambda x: np.std(x)/np.mean(x))\n",
    "train_freq2mean = train_freq2.groupby(['FileID'])['Delta_sd'].mean().fillna(0).reset_index()\n",
    "train_freq2max = train_freq2.groupby(['FileID'])['Delta_sd'].max().fillna(0).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_freq1.shape, train_freq2.shape, train_dum.shape, train_agg.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2B. 資料清整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部合併\n",
    "\n",
    "train_all = pd.merge(train_freq1mean, train_freq1max, on=['FileID'], how='outer')\n",
    "train_all = pd.merge(train_all, train_freq2mean, on=['FileID'], how='outer')\n",
    "train_all = pd.merge(train_all, train_freq2max, on=['FileID'], how='outer')\n",
    "train_all = pd.merge(train_all, train_dum, on=['FileID'], how='outer')\n",
    "train_all = pd.merge(train_all, train_agg, on=['FileID'], how='outer')\n",
    "\n",
    "train_all.head()\n",
    "\n",
    "# index & column 相merge時: train_all = pd.merge(train_all, train_agg, how='left', left_on=['FileID'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_all.columns)\n",
    "train_all.to_csv(\"train_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去除 outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 補值 (interval: 同一file同一customer之間的時間差)\n",
    "\n",
    "train_all[['FMin']]     = train_all[['FMin']].fillna(value=0) # 沒有interval\n",
    "train_all[['Delta_sd']] = train_all[['Delta_sd']].fillna(value=100) # 沒有兩個以上interval\n",
    "\n",
    "check1 = len(train_all[train_all['FMin'].isnull()])\n",
    "check2 = len(train_all[train_all['Delta_sd'].isnull()])\n",
    "check3 = len(train_all[train_all['20f8a5'].isnull()])\n",
    "check4 = len(train_all[train_all['DayTCsMea'].isnull()])\n",
    "\n",
    "print(check1)\n",
    "print(check2)\n",
    "print(check3)\n",
    "print(check4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "'''\n",
    "def normalize(x, axis, method, minmax_range =(0,1)):\n",
    "    if method == 'z-score':\n",
    "        scale_a = preprocessing.scale(a, axis=axis)\n",
    "    elif method== 'minmax':    \n",
    "        scale_a = preprocessing.minmax_scale(a, axis=axis, feature_range=minmax_range) #default feature range 0~1\n",
    "    return scale_a\n",
    "axis =0\n",
    "scale_a1 = normalize(a, axis, method = 'z-score')\n",
    "scale_a2 = normalize(a, axis, method = 'minmax', minmax_range=(0,1))\n",
    "print(scale_a1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 探索性資料分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散布圖\n",
    "'''\n",
    "color = \"rbg\"\n",
    "color = [color[y[i]] for i in range(len(y))]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('Actual')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=prediction)\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 長條圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代 train model - 避免imbalance (test data不用拆，直接看AUC&ROC分數即可)\n",
    "\n",
    "def train_batch_generator(x, y, bs):\n",
    "    badIndex = y[y == 1].index\n",
    "    goodIndex = y[y == 0].index\n",
    "\n",
    "    while(True):\n",
    "        newBad_ind = shuffle(badIndex)\n",
    "        newgood_ind = shuffle(goodIndex)\n",
    "\n",
    "        newBad_ind = newBad_ind[:int(bs/2)]\n",
    "        newgood_ind = newgood_ind[:int(bs/2)]\n",
    "\n",
    "        batch_x = x.loc[newBad_ind]\n",
    "        batch_y = y.loc[newBad_ind]\n",
    "\n",
    "        batch_x = batch_x.append(x.loc[newgood_ind], ignore_index=True)\n",
    "        batch_y = batch_y.append(y.loc[newgood_ind], ignore_index=True)\n",
    "\n",
    "        yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分成 train & test\n",
    "\n",
    "train_lbl = pd.merge(train_all, train, on=['FileID'], how='left')\n",
    "\n",
    "train_all = train_all.sort_values(['FileID'])\n",
    "train_lbl = train_lbl.sort_values(['FileID'])\n",
    "\n",
    "X = train_all.drop(['FileID','533133','218578','262880'], axis=1)\n",
    "y = train_lbl['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 68, shuffle=False)\n",
    "\n",
    "gen = train_batch_generator(X_train, y_train, 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e8e83807125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(X_train, y_train, \n\u001b[1;32m      4\u001b[0m           \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "    \n",
    "model.fit(X_train, y_train, \n",
    "          eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "          eval_metric='auc',\n",
    "          verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_modl = model.predict(bat_x) # 找 bat_x 的auc score\n",
    "y_pred = model.predict(X_test)# 找 X_test 的auc score\n",
    "\n",
    "#y_pred = model.predict_proba(X_test)[:,1] # 找 X_test 的auc score\n",
    "\n",
    "auc1    = metrics.roc_auc_score(bat_y, y_modl)\n",
    "auc2    = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(auc1, auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bat_x, bat_y = next(gen)\n",
    "print(bat_x, bat_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LASSO / Ridge\n",
    "\n",
    "model = Lasso(alpha=1, normalize=False, copy_X=True, fit_intercept=True, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X)\n",
    "print('accuracy:')\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "model = Ridge(alpha=1, normalize=False, copy_X=True, fit_intercept=True, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X)\n",
    "print('accuracy:')\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "model = SVC(kernel='poly',coef0=0, degree=3)\n",
    "model.fit(X_train, y_train)\n",
    "#print('accuracy:', model.accuracy_score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.516508391608 0.498755185335\n",
      "0.515652797203 0.503177201456\n",
      "0.51476013986 0.501870563781\n",
      "0.516071328671 0.498903059292\n",
      "0.517938111888 0.498438829752\n",
      "0.51476013986 0.501891171451\n",
      "0.512574825175 0.500099696566\n",
      "0.512574825175 0.499773037147\n",
      "0.514323076923 0.501791474885\n",
      "0.516882517483 0.497242192478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfig, ax = pyplot.subplots()\\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\\nax.legend()\\npyplot.ylabel('Log Loss')\\npyplot.title('XGBoost Log Loss')\\npyplot.show()\\n\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "\n",
    "for i in range(10):\n",
    "    bat_x, bat_y = next(gen)\n",
    "    \n",
    "    model.fit(bat_x, bat_y, \n",
    "              eval_set=[(bat_x, bat_y), (X_test, y_test)],\n",
    "              eval_metric='auc',\n",
    "              verbose=False)\n",
    "    \n",
    "    y_modl = model.predict(bat_x) # 找 bat_x 的auc score\n",
    "    y_pred = model.predict(X_test)# 找 X_test 的auc score\n",
    "    \n",
    "    #y_pred = model.predict_proba(X_test)[:,1] # 找 X_test 的auc score\n",
    "    \n",
    "    auc1    = metrics.roc_auc_score(bat_y, y_modl)\n",
    "    auc2    = metrics.roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(auc1, auc2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "eval_set = [(X_test, y_test)]\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print metrics.roc_auc_score(y_test,y_pred)\n",
    "auc_score = roc_auc_score(y_test, pipe.predict_proba(X_test)[:,1]))\n",
    "\n",
    "if auc_score < 0.5:\n",
    "    fpr_svc, tpr_svc, _ = roc_curve(y_test, pipe.predict_proba(X_test)[:,1], pos_label=0)\n",
    "    auc_score = 1 - auc_score\n",
    "else:\n",
    "    fpr_svc, tpr_svc, _ = roc_curve(y_test, pipe.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.plot(fpr_svc, tpr_svc, label='ROC Curve', color='cyan')\n",
    "plt.plot([0,1], [0,1], color='black', linestyle='--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "'''\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "# eval_metric 選項: rmse, logloss, error, auc, merror, mlogloss or custom define\n",
    "\n",
    "# model.predict_proba will return the probility\n",
    "# model.predict will return the predict label (use 0.5 as threshold)\n",
    "y_pred = model.predict_proba(X_test)\n",
    "y_pred[:5]\n",
    "\n",
    "# we use model.predict to get the label\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred )\n",
    "print(\"Accuracy:\" % (accuracy * 100.0))\n",
    "\n",
    "# we can show the feature importances for our features\n",
    "print(model.feature_importances_)\n",
    "# import the plot_importance function to visualize the feature importance\n",
    "from xgboost import plot_importance\n",
    "plot_importance(model)\n",
    "plt.show()\n",
    "\n",
    "from xgboost import plot_tree\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "plot_tree(model, num_trees=1)\n",
    "# plt.title(\"max_depth = 100, with gamma = 10\")\n",
    "# plt.savefig(\"tree_with_max_depth_gamma\", dpi = 700)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. 組合模型 (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. 上傳結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3536571a9bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
